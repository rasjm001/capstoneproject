import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from xgboost import XGBClassifier
from art.attacks.evasion import FastGradientMethod
from art.estimators.classification import SklearnClassifier

# Load the dataset from a CSV file
df = pd.read_csv('../data.csv')  # Adjust the path as needed

# Encode class labels (Benign: 0, Malware: 1) using LabelEncoder
df["Class"] = LabelEncoder().fit_transform(df["Class"])

# Compute the mean of svcscan.nservices for benign samples (Class = 0)
benign_mean_svcscan = df[df["Class"] == 0]["svcscan.nservices"].mean()
print(f"Mean svcscan.nservices for benign samples: {benign_mean_svcscan}")

# Define features (X) and target (y)
# Preserve "Category" column for later use if it exists
if "Category" in df.columns:
    category_col = df["Category"]
X = df.drop(columns=["Category", "Class"] if "Category" in df.columns else ["Class"])
y = df["Class"]

# Split the data into training (80%) and testing (20%) sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# If "Category" exists, split it accordingly
if "Category" in df.columns:
    category_train, category_test = train_test_split(category_col, test_size=0.2, random_state=42, stratify=y)

# Preserve the original training data (unscaled)
X_train_original = X_train.copy()

# Scale the features using StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_original)
X_test_scaled = scaler.transform(X_test)

# Define classifiers
classifiers = {
    "Logistic Regression": LogisticRegression(max_iter=10000),
    "Random Forest": RandomForestClassifier(n_estimators=50),
    "SVM": SVC(kernel="linear", probability=True),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric="logloss")
}

# Train and store models
trained_models = {}
for name, clf in classifiers.items():
    clf.fit(X_train_scaled, y_train)
    y_pred = clf.predict(X_test_scaled)
    trained_models[name] = clf
    print(f"\nClassifier: {name}")
    print(classification_report(y_test, y_pred, target_names=["Benign", "Malware"]))

# Select 100 malware samples from the test set
malware_indices = np.where(y_test == 1)[0]
selected_indices = np.random.choice(malware_indices, 100, replace=False)
X_test_malware = X_test.iloc[selected_indices]
y_test_malware = y_test.iloc[selected_indices]

# If "Category" exists, select corresponding categories
if "Category" in df.columns:
    category_malware = category_test.iloc[selected_indices]

# Modify svcscan.nservices for adversarial malware samples
X_test_malware_raw = X_test_malware.copy()
X_test_malware_raw["svcscan.nservices"] = benign_mean_svcscan
X_test_malware_scaled = scaler.transform(X_test_malware_raw)

# Define features to perturb
features_to_perturb = ['pslist.nproc', 'malfind.ninjections', 'handles.nhandles', 'dlllist.ndlls', 'svcscan.kernel_drivers']

# Generate adversarial samples using ART
classifier = SklearnClassifier(model=trained_models["Logistic Regression"])
fgsm = FastGradientMethod(estimator=classifier, eps=0.15)
X_test_adv_scaled = fgsm.generate(X_test_malware_scaled)

# Compute perturbation and restrict it to specified features
perturbation = X_test_adv_scaled - X_test_malware_scaled
indices_not_to_perturb = [X.columns.get_loc(feat) for feat in X.columns if feat not in features_to_perturb]
perturbation[:, indices_not_to_perturb] = 0
X_test_adv_scaled = X_test_malware_scaled + perturbation

# Inverse transform to raw space
X_test_adv_raw = scaler.inverse_transform(X_test_adv_scaled)
X_test_malware_raw_original = scaler.inverse_transform(X_test_malware_scaled)

# Create DataFrame for adversarial samples in the original dataset format
adv_samples_df = pd.DataFrame(X_test_adv_raw, columns=X.columns)
adv_samples_df["Class"] = "Malware"  # Label as Malware since these are adversarial malware samples
if "Category" in df.columns:
    adv_samples_df.insert(0, "Category", category_malware.values)

# Save adversarial samples to CSV
adv_samples_df.to_csv('adversarial_samples_specific_features.csv', index=False)
print("\nAdversarial samples saved to 'adversarial_samples_specific_features.csv'")

# Create DataFrame for comparison (original and adversarial)
comparison_df = pd.DataFrame(X_test_malware_raw_original, columns=X.columns)
comparison_df['Label'] = y_test_malware.values
comparison_df['Type'] = 'Original'
adv_comparison_df = pd.DataFrame(X_test_adv_raw, columns=X.columns)
adv_comparison_df['Label'] = y_test_malware.values
adv_comparison_df['Type'] = 'Adversarial'
comparison_samples_df = pd.concat([comparison_df, adv_comparison_df], ignore_index=True)

# Save comparison CSV
comparison_samples_df.to_csv('adversarial_sample_comparison_specific_features.csv', index=False)
print("\nComparison of original and adversarial samples saved to 'adversarial_sample_comparison_specific_features.csv'")

# Evaluate the adversarial samples on all trained models
for name, clf in trained_models.items():
    y_pred_adv = clf.predict(X_test_adv_scaled)
    print(f"\nClassifier: {name} (Adversarial)")
    unique_classes = np.unique(y_test_malware)
    if len(unique_classes) == 1:
        print(f"Warning: Only one class ({unique_classes[0]}) present in y_test_malware.")
        print(classification_report(y_test_malware, y_pred_adv, labels=unique_classes, target_names=["Malware"] if 1 in unique_classes else ["Benign"]))
    else:
        print(classification_report(y_test_malware, y_pred_adv, target_names=["Benign", "Malware"]))
    evasion_indices = np.where((y_pred_adv == 0) & (y_test_malware == 1))[0]
    print(f"Number of adversarial samples evading detection: {len(evasion_indices)}")